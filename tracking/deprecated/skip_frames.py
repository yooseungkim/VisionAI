import math
import os
import time
import threading
from queue import Queue

import cv2
import numpy as np
import torch
from ultralytics import YOLO
import subprocess

# --- Configuration ---
video_name = "parking3.mp4"
video_path = f"datasets/{video_name}"
os.makedirs("results", exist_ok=True)
final_output_path = f"results/temp_extreme_speed_{video_name}"
temp_output_path = f"results/extreme_speed_{video_name}"

# [ì„±ëŠ¥ í•µì‹¬]
BATCH_SIZE = 16          # 16ì¥ì”© GPU ì²˜ë¦¬
SKIP_LOGIC_FRAMES = 16   # 16í”„ë ˆì„ ì¤‘ 1ê°œë§Œ ë¡œì§ ìˆ˜í–‰ (BATCH_SIZEì™€ ë§ì¶”ëŠ” ê²ƒ ê¶Œì¥)
MODEL_NAME = "yolo11m-seg.pt"

# Logic Params
CONF_THRESHOLD = 0.5
SMOOTH_WINDOW = 5        # ë¡œì§ ìˆ˜í–‰ ë¹ˆë„ê°€ ë‚®ìœ¼ë¯€ë¡œ ìœˆë„ìš° ì¤„ì„
REID_SIMILARITY_THRESH = 0.80
MAX_LOST_FRAMES = 300    # ìŠ¤í‚µì´ ë§ìœ¼ë¯€ë¡œ ë„‰ë„‰í•˜ê²Œ

# --- Simplified Helper Classes ---
class VideoCaptureThread:
    def __init__(self, path, queue_size=128):
        self.cap = cv2.VideoCapture(path)
        if not self.cap.isOpened(): raise ValueError(f"Open Error: {path}")
        self.q = Queue(maxsize=queue_size)
        self.stopped = False
        self.fps = self.cap.get(cv2.CAP_PROP_FPS)
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.t = threading.Thread(target=self._reader, daemon=True)
        self.t.start()

    def _reader(self):
        while not self.stopped:
            if self.q.full():
                time.sleep(0.01)
                continue
            ret, frame = self.cap.read()
            if not ret:
                self.stopped = True
                break
            self.q.put(frame)

    def read_batch(self, batch_size):
        frames = []
        if self.stopped and self.q.empty(): return []
        for _ in range(batch_size):
            if self.q.empty():
                if self.stopped: break
                time.sleep(0.001)
                if self.q.empty(): break
            frames.append(self.q.get())
        return frames

    def release(self):
        self.stopped = True
        self.t.join()
        self.cap.release()

# ReIDë„ ìµœì†Œí™”
class SimpleReID:
    def __init__(self):
        self.known_hists = {}
        self.id_map = {}
        self.next_vid = 1
    
    def get_hist(self, img, mask, box):
        x, y, w, h = map(int, box)
        x=max(0,x); y=max(0,y); w=min(w,img.shape[1]-x); h=min(h,img.shape[0]-y)
        if w<=0 or h<=0: return None
        roi = img[y:y+h, x:x+w]
        m = np.zeros(roi.shape[:2], dtype=np.uint8)
        cnt = mask - [x,y]
        cv2.drawContours(m, [cnt.astype(np.int32)], -1, 255, -1)
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        hist = cv2.calcHist([hsv], [0, 1], m, [18, 20], [0, 180, 0, 256])
        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)
        return hist

    def update(self, yolo_id, img, mask, box):
        if yolo_id in self.id_map: return self.id_map[yolo_id]
        
        # ìƒˆ IDì¸ ê²½ìš°ì—ë§Œ ë¬´ê±°ìš´ ì—°ì‚° ìˆ˜í–‰
        hist = self.get_hist(img, mask, box)
        if hist is None: return self.next_vid
        
        # ë§¤ì¹­ ë¡œì§ (ìƒëµí•˜ê±°ë‚˜ ê°„ì†Œí™”)
        # ì†ë„ë¥¼ ìœ„í•´ ê°€ì¥ ë‹¨ìˆœí•˜ê²Œ: ìƒˆ ID ë¶€ì—¬
        # ì •êµí•œ ë§¤ì¹­ì´ í•„ìš”í•˜ë©´ ì—¬ê¸° ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©
        vid = self.next_vid
        self.next_vid += 1
        self.id_map[yolo_id] = vid
        return vid

# --- Main Setup ---
cap_thread = VideoCaptureThread(video_path)
FPS = cap_thread.fps
w, h = cap_thread.width, cap_thread.height

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = YOLO(MODEL_NAME).to(device)

reid = SimpleReID()
state_map = {} # {vid: "Stopped" or "Moving"}

# ì €ì¥ FPS ì„¤ì • (ì‹¤ì‹œê°„ ì¬ìƒ ì†ë„ ë§ì¶¤)
SAVE_FPS = FPS / BATCH_SIZE
out = cv2.VideoWriter(temp_output_path, cv2.VideoWriter_fourcc(*'mp4v'), SAVE_FPS, (w, h))

print(f"ğŸ”¥ EXTREME SPEED MODE STARTED | Batch: {BATCH_SIZE}")

start_time = time.time()
processed_cnt = 0

try:
    while True:
        # 1. ë°°ì¹˜ ì½ê¸° (CPU Thread)
        batch = cap_thread.read_batch(BATCH_SIZE)
        if not batch: break
        curr_batch_len = len(batch)

        # 2. ë°°ì¹˜ ì¶”ë¡  (GPU) - ì—¬ê¸°ì„œ ì¶”ì (ID ìœ ì§€)ì€ ë‚´ë¶€ì ìœ¼ë¡œ ë‹¤ ë¨
        results = model.track(batch, persist=True, verbose=False, retina_masks=True)

        # 3. [í•µì‹¬] ë§ˆì§€ë§‰ í”„ë ˆì„ë§Œ ì²˜ë¦¬!
        # ì¤‘ê°„ í”„ë ˆì„ì€ ë²„ë¦½ë‹ˆë‹¤. (íŒŒì´ì¬ ë£¨í”„ ì‚­ì œ)
        last_idx = curr_batch_len - 1
        result = results[last_idx]
        frame = batch[last_idx]

        if result.boxes and result.boxes.id is not None:
            boxes = result.boxes.xywh.cpu().numpy()
            ids = result.boxes.id.int().cpu().tolist()
            masks = result.masks.xy if result.masks is not None else []
            
            mask_overlay = frame.copy()
            
            for i, yolo_id in enumerate(ids):
                box = boxes[i]
                mask = masks[i]
                if mask is None or len(mask) == 0: continue

                # ReID (ë§ˆì§€ë§‰ í”„ë ˆì„ì— ëŒ€í•´ì„œë§Œ ìˆ˜í–‰)
                vid = reid.update(yolo_id, frame, mask, box)
                
                # ì‹œê°í™” (ë°”ë¡œ ê·¸ë¦¬ê¸°)
                color = (0, 255, 0) # Green
                cv2.fillPoly(mask_overlay, [mask.astype(np.int32)], color)
                cv2.polylines(frame, [mask.astype(np.int32)], True, color, 2)
                
                # í…ìŠ¤íŠ¸
                bx, by, bw, bh = box
                cv2.putText(frame, f"ID:{vid}", (int(bx), int(by)), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)

            cv2.addWeighted(mask_overlay, 0.4, frame, 0.6, 0, frame)

        # 4. ì €ì¥ (1ì¥ë§Œ)
        out.write(frame)
        
        processed_cnt += curr_batch_len
        
        if processed_cnt % (BATCH_SIZE * 5) == 0:
            elapsed = time.time() - start_time
            print(f"Processed {processed_cnt} frames. FPS: {processed_cnt/elapsed:.2f}")

except KeyboardInterrupt:
    print("Stopped.")

cap_thread.release()
out.release()

print(f"âœ… Temporary video saved: {temp_output_path}")

# --- [ì¶”ê°€ëœ ë¶€ë¶„] FFmpeg ë³€í™˜ ë¡œì§ ---
print("â³ Converting to H.264 using FFmpeg...")

if os.path.exists(temp_output_path):
    command = [
        "ffmpeg", "-y",                 # -y: ë®ì–´ì“°ê¸° í—ˆìš©
        "-i", temp_output_path,         # ì…ë ¥ íŒŒì¼
        "-vcodec", "libx264",           # H.264 ì½”ë± ì‚¬ìš©
        "-crf", "23",                   # í™”ì§ˆ ì„¤ì • (ë‚®ì„ìˆ˜ë¡ ê³ í™”ì§ˆ, 23ì€ ê¸°ë³¸ê°’)
        "-preset", "fast",              # ì¸ì½”ë”© ì†ë„ ì„¤ì •
        "-an",                          # ì˜¤ë””ì˜¤ ì œê±° (CCTVë¼ ë¶ˆí•„ìš”)
        final_output_path               # ì¶œë ¥ íŒŒì¼
    ]
    
    try:
        # ffmpeg ì‹¤í–‰ (ë¡œê·¸ ìˆ¨ê¹€: capture_output=True)
        subprocess.run(command, check=True)
        print(f"ğŸ‰ Conversion Complete! Saved to: {final_output_path}")
        
        # (ì„ íƒì‚¬í•­) ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_output_path)
        print("ğŸ—‘ï¸  Temporary file removed.")
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ FFmpeg Error: {e}")
    except FileNotFoundError:
        print("âŒ FFmpeg not found. Please install ffmpeg (sudo apt install ffmpeg).")
else:
    print("âŒ Error: Temporary file not found.")